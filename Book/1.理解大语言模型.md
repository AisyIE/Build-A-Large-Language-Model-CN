本章涵盖以下内容：

- **大语言模型（LLMs）背后基本概念的高级解释**
- **对大语言模型（如 ChatGPT 平台上使用的模型）所源自的 Transformer 架构的深入了解**
- **从零开始构建大语言模型的计划**



大型语言模型 (LLMs)，如 OpenAI 的 ChatGPT，是近年来发展起来的深度神经网络模型。这些模型为自然语言处理 (NLP) 开辟了一个新时代。在大型语言模型出现之前，传统方法在电子邮件垃圾分类等分类任务中表现良好，但通常在需要复杂理解和生成能力的语言任务上表现不佳，例如解析详细指令、进行上下文分析，或生成连贯且符合上下文的原创文本。例如，早期的语言模型无法根据关键词列表撰写电子邮件，而这个任务对现代 LLMs 来说却非常简单。

​		LLMs 具备理解、生成和解释人类语言的卓越能力。然而，我们需要澄清的是，当我们说语言模型“理解”时，并不是说它们具有人类的意识或理解能力，而是指它们能够以看起来连贯且符合上下文的方式处理和生成文本

​		得益于深度学习的进展，深度学习是机器学习和人工智能 (AI) 的一个子集，主要关注神经网络，LLMs 在海量文本数据上进行训练。这使得 LLMs 能够捕捉到比以往方法更深层的上下文信息和人类语言的细微差别。因此，LLMs 在各种自然语言处理 (NLP) 任务中的表现得到了显著提升，包括文本翻译、情感分析、问答等。

​		当代 LLMs 与早期 NLP 模型之间的另一个重要区别在于，早期的 NLP 模型通常是为特定任务而设计的，例如文本分类、语言翻译等。虽然这些早期模型在其特定应用中表现出色，但 LLMs 在各种自然语言处理 (NLP) 任务中展现了更广泛的能力。

​		LLMs 的成功可以归因于支撑 LLMs 的 transformer 架构，以及 LLMs 训练所用的海量数据。这使得它们能够捕捉到多种语言的细微差别、上下文和模式，而这些都是难以手动编码的。

​		这种转向基于 transformer 架构的模型和大规模训练数据集来训练 LLMs，已经从根本上改变了自然语言处理 (NLP) 领域，为理解和与人类语言互动提供了更强大的工具。

​		从本章开始，我们将奠定实现本书主要目标的基础：通过逐步在代码中实现一个基于 transformer 架构的类似 ChatGPT 的 LLM，以帮助理解 LLMs。



## 1.1 LLM 是什么？







